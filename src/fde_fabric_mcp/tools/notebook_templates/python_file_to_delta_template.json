{
  "description": "Starter notebook for ingesting lakehouse bronze files into a silver Delta table with optional date partition reads/writes, dry-run mode, multithreading across days, and restate scaffolding.",
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lakehouse Bronze → Silver Delta Ingest Template\n",
        "\n",
        "This notebook is a **minimal, customizable Fabric ingestion template**:\n",
        "\n",
        "- **Reads bronze lakehouse files** from OneLake (date-partitioned or non-partitioned)\n",
        "- **Generates a DataFrame** for each day (your customization point)\n",
        "- **Writes to a silver Delta table** (partitioned or non-partitioned)\n",
        "- Supports **DRY_RUN** mode (display the DataFrame instead of writing)\n",
        "- Supports **multi-day restates** with optional multithreading\n",
        "- Returns an **orchestrator-safe JSON payload** on exit\n",
        "\n",
        "## Common usage patterns\n",
        "\n",
        "**Daily run (one day):**\n",
        "- `RESTATE=False`\n",
        "- `DATA_DAY` derived from `input_data_interval_utc`\n",
        "\n",
        "**Range restate (many days):**\n",
        "- `RESTATE=True`\n",
        "- uses `START_RANGE` → `END_RANGE`\n",
        "\n",
        "**Bronze layout examples**\n",
        "\n",
        "### Date-partitioned bronze\n",
        "```\n",
        "/Files/bronze/dayforce/employee/\n",
        "  year=2025/month=11/day=02/\n",
        "    part-0000.json\n",
        "    _run_manifest/2025-11-02.json\n",
        "```\n",
        "\n",
        "### Non-partitioned bronze\n",
        "```\n",
        "/Files/bronze/dayforce/employee/\n",
        "  part-0000.json\n",
        "  part-0001.json\n",
        "```\n",
        "\n",
        "**Silver write examples**\n",
        "\n",
        "### Partitioned silver\n",
        "- `WRITE_SILVER_PARTITIONED=True`\n",
        "- `SILVER_PARTITION_COLS=[\"update_day_utc\"]`\n",
        "\n",
        "### Non-partitioned silver\n",
        "- `WRITE_SILVER_PARTITIONED=False`\n",
        "- `SILVER_PARTITION_COLS` ignored\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Parameters & toggles\n",
        "\n",
        "Fabric pipelines typically inject parameters at runtime. This template supports:\n",
        "\n",
        "- `input_stage`: environment (dev/test/prod)\n",
        "- `run_interval_utc`: orchestrator run timestamp\n",
        "- `input_data_interval_utc`: data timestamp used to derive the daily slice\n",
        "\n",
        "### Toggle highlights\n",
        "\n",
        "- **Read bronze with/without date partitioning** via `READ_BRONZE_DATE_PARTITIONED`\n",
        "- **Write silver with/without partitioning** via `WRITE_SILVER_PARTITIONED`\n",
        "- **DRY_RUN** to validate schemas/transforms without writing Delta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters injected from the pipeline/runtime\n",
        "from datetime import datetime\n",
        "from notebookutils import runtime\n",
        "\n",
        "# --------- Runtime parameters (Fabric will inject these) ----------\n",
        "run_interval_utc        = globals().get(\"run_interval_utc\", \"2026-01-03 07:00:00\")\n",
        "input_data_interval_utc = globals().get(\"input_data_interval_utc\", \"2026-01-02 07:00:00\")\n",
        "input_stage             = globals().get(\"input_stage\", \"dev\")\n",
        "\n",
        "# --------- Processing toggles ----------\n",
        "PARALLEL_DAYS = True\n",
        "MAX_WORKERS   = 4\n",
        "\n",
        "# Daily slice (used when RESTATE=False)\n",
        "DATA_DAY = datetime.strptime(input_data_interval_utc[:10], \"%Y-%m-%d\")\n",
        "\n",
        "# Range restate mode\n",
        "RESTATE     = True\n",
        "START_RANGE = \"2026-01-01\"\n",
        "END_RANGE   = \"2026-01-02\"\n",
        "\n",
        "# --------- Read options ----------\n",
        "# True  => reads .../year=YYYY/month=MM/day=DD/\n",
        "# False => reads part-* files directly under BRONZE_BASE\n",
        "READ_BRONZE_DATE_PARTITIONED = True\n",
        "\n",
        "# Manifest gating (recommended for partitioned bronze drops)\n",
        "REQUIRE_MANIFEST = True\n",
        "\n",
        "# --------- Write options ----------\n",
        "WRITE_MODE = \"overwrite\"   # \"overwrite\" or \"append\"\n",
        "\n",
        "# True  => partition_by SILVER_PARTITION_COLS\n",
        "# False => write non-partitioned Delta\n",
        "WRITE_SILVER_PARTITIONED = True\n",
        "SILVER_PARTITION_COLS    = [\"update_day_utc\"]\n",
        "\n",
        "# --------- DRY RUN ----------\n",
        "# True => display dataframe and skip writing\n",
        "DRY_RUN = False\n",
        "DRY_RUN_SHOW_ROWS   = 50\n",
        "DRY_RUN_SHOW_SCHEMA = True\n",
        "\n",
        "# --------- Project Vars ----------\n",
        "PROJECT_GROUP = \"dayforce\"\n",
        "PROJECT_NAME  = \"employee_updates_daily\"\n",
        "BRONZE_ENTITY = \"employee\"   # folder name under /Files/bronze/<group>/<entity>\n",
        "TARGET_CLASS  = \"silver\"\n",
        "\n",
        "PROJECT_ID = runtime.context.get(\"currentNotebookId\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Unified helper functions\n",
        "\n",
        "These helpers are intended to be **copied across ingestion notebooks** and reused:\n",
        "\n",
        "- Path builders for OneLake lakehouse locations\n",
        "- Safe listing and manifest validation\n",
        "- Simple part-file reader (`.json` + `.parquet`)\n",
        "- Delta write helper supporting partitioned and non-partitioned outputs\n",
        "\n",
        "### Table layout\n",
        "\n",
        "**Bronze root:**\n",
        "```\n",
        "abfss://fde_core_data_<stage>@onelake.dfs.fabric.microsoft.com/core_lh.Lakehouse/Files/bronze/<group>/<entity>/\n",
        "```\n",
        "\n",
        "**Silver table path:**\n",
        "```\n",
        "abfss://fde_core_data_<stage>@onelake.dfs.fabric.microsoft.com/core_lh.Lakehouse/Tables/silver/<group>_<project>\n",
        "```\n",
        "\n",
        "### Partition write example\n",
        "\n",
        "- Partitioned by day:\n",
        "  - `WRITE_SILVER_PARTITIONED=True`\n",
        "  - `SILVER_PARTITION_COLS=[\"update_day_utc\"]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import threading\n",
        "from typing import List, Optional\n",
        "\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "from deltalake import write_deltalake\n",
        "import notebookutils\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "# Reused lock to avoid concurrent Delta writes to the same path.\n",
        "_DELTA_WRITE_LOCK = threading.Lock()\n",
        "\n",
        "def lakehouse_root(stage: str) -> str:\n",
        "    return f\"abfss://fde_core_data_{stage}@onelake.dfs.fabric.microsoft.com/core_lh.Lakehouse\"\n",
        "\n",
        "def bronze_base_path(stage: str, project_group: str, entity: str) -> str:\n",
        "    return f\"{lakehouse_root(stage)}/Files/bronze/{project_group}/{entity}\"\n",
        "\n",
        "def bronze_day_path(base: str, data_day: datetime) -> str:\n",
        "    return f\"{base}/year={data_day:%Y}/month={data_day:%m}/day={data_day:%d}/\"\n",
        "\n",
        "def silver_table_path(stage: str, classification: str, project_group: str, project: str) -> str:\n",
        "    return f\"{lakehouse_root(stage)}/Tables/{classification}/{project_group}_{project}\"\n",
        "\n",
        "def _ls_safe(path: str):\n",
        "    try:\n",
        "        return notebookutils.fs.ls(path)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def validate_manifest(day_path: str, data_day: datetime) -> bool:\n",
        "    \"\"\"\n",
        "    Validates a bronze run manifest:\n",
        "      <day_path>/_run_manifest/<YYYY-MM-DD>.json\n",
        "    \"\"\"\n",
        "    manifest_dir  = f\"{day_path}_run_manifest/\"\n",
        "    manifest_name = f\"{data_day:%Y-%m-%d}.json\"\n",
        "\n",
        "    entries = _ls_safe(manifest_dir)\n",
        "    if entries is None:\n",
        "        logging.info(\"[SKIP] Manifest folder missing: %s\", manifest_dir)\n",
        "        return False\n",
        "\n",
        "    names = {e.name for e in entries}\n",
        "    if manifest_name not in names:\n",
        "        logging.info(\"[SKIP] Manifest missing: %s%s\", manifest_dir, manifest_name)\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def list_part_files(path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Lists files directly in `path` and returns part-*.json / part-*.parquet.\n",
        "    (Non-recursive by design.)\n",
        "    \"\"\"\n",
        "    entries = _ls_safe(path)\n",
        "    if not entries:\n",
        "        return []\n",
        "\n",
        "    out = []\n",
        "    for e in entries:\n",
        "        n = e.name.lower()\n",
        "        if n.startswith(\"part-\") and (n.endswith(\".json\") or n.endswith(\".parquet\")):\n",
        "            out.append(path + e.name)\n",
        "    return out\n",
        "\n",
        "def read_bronze_parts_as_df(part_files: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"Simple reader supporting JSON and Parquet part files.\"\"\"\n",
        "    if not part_files:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    dfs = []\n",
        "    for p in part_files:\n",
        "        pl = p.lower()\n",
        "        if pl.endswith(\".json\"):\n",
        "            dfs.append(pd.read_json(p))\n",
        "        elif pl.endswith(\".parquet\"):\n",
        "            dfs.append(pd.read_parquet(p))\n",
        "        else:\n",
        "            logging.warning(\"Unknown file type: %s (skipping)\", p)\n",
        "\n",
        "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
        "\n",
        "def save_df_to_delta(\n",
        "    df: pd.DataFrame,\n",
        "    table_path: str,\n",
        "    *,\n",
        "    partition_columns: Optional[List[str]] = None,\n",
        "    mode: str = \"overwrite\",\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Writes a pandas DataFrame to a Delta table path.\n",
        "\n",
        "    - If `partition_columns` is None => writes a non-partitioned Delta table\n",
        "    - If `partition_columns` is a list => writes partitioned Delta\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        logging.warning(\"No rows to write to %s\", table_path)\n",
        "        return\n",
        "\n",
        "    storage_options = {\n",
        "        \"bearer_token\": notebookutils.credentials.getToken(\"storage\"),\n",
        "        \"use_fabric_endpoint\": \"true\",\n",
        "    }\n",
        "\n",
        "    arrow_tbl = pa.Table.from_pandas(df.reset_index(drop=True), preserve_index=False)\n",
        "\n",
        "    with _DELTA_WRITE_LOCK:\n",
        "        write_deltalake(\n",
        "            table_path,\n",
        "            arrow_tbl,\n",
        "            mode=mode,\n",
        "            partition_by=(partition_columns or None),\n",
        "            storage_options=storage_options,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Generate the silver DataFrame (customization point)\n",
        "\n",
        "This is the **only function most projects need to modify**.\n",
        "\n",
        "Responsibilities:\n",
        "\n",
        "1. Resolve the bronze read path for the day\n",
        "2. Gate on manifest (optional)\n",
        "3. Load part files into a DataFrame\n",
        "4. Apply light standardization (column naming, required columns)\n",
        "5. Return the DataFrame to be written to silver\n",
        "\n",
        "### Example customizations\n",
        "\n",
        "- Convert nested structs to JSON strings before writing\n",
        "- Cast date/timestamp columns\n",
        "- Deduplicate by a business key\n",
        "- Align output columns to an expected schema\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_silver_df_for_day(data_day: datetime) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Minimal bronze → silver day slice.\n",
        "\n",
        "    Reads bronze part files for a given day (or base path if non-partitioned)\n",
        "    and returns a DataFrame ready to write.\n",
        "    \"\"\"\n",
        "    base = bronze_base_path(input_stage, PROJECT_GROUP, BRONZE_ENTITY)\n",
        "\n",
        "    # Decide where to read from\n",
        "    if READ_BRONZE_DATE_PARTITIONED:\n",
        "        day_path = bronze_day_path(base, data_day)\n",
        "        if REQUIRE_MANIFEST and not validate_manifest(day_path, data_day):\n",
        "            return pd.DataFrame()\n",
        "        read_path = day_path\n",
        "    else:\n",
        "        read_path = base\n",
        "\n",
        "    part_files = list_part_files(read_path)\n",
        "    if not part_files:\n",
        "        logging.info(\"[SKIP] No part-* files found at %s\", read_path)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    logging.info(\"[LOAD] %s part file(s) from %s\", len(part_files), read_path)\n",
        "    df = read_bronze_parts_as_df(part_files)\n",
        "\n",
        "    if df.empty:\n",
        "        logging.info(\"[SKIP] Read 0 rows for %s\", data_day.strftime(\"%Y-%m-%d\"))\n",
        "        return df\n",
        "\n",
        "    # --- Minimal standardization hooks ---\n",
        "    # 1) normalize column names (optional)\n",
        "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
        "\n",
        "    # 2) add/update a day column for downstream partitioning & joins\n",
        "    df[\"update_day_utc\"] = pd.to_datetime(data_day.strftime(\"%Y-%m-%d\")).date()\n",
        "\n",
        "    # 3) project-specific transforms go here\n",
        "    # Example: df = df.drop_duplicates([\"employeeid\", \"update_day_utc\"])\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Ingestion logic & orchestration (multi-day + DRY_RUN)\n",
        "\n",
        "This cell wires together:\n",
        "\n",
        "- `process_day()`\n",
        "  - calls `generate_silver_df_for_day()`\n",
        "  - **DRY_RUN**: displays the DataFrame instead of writing\n",
        "  - otherwise writes to silver (partitioned or non-partitioned)\n",
        "- `run_core_logic()`\n",
        "  - decides whether to run a single day or a date range\n",
        "  - optionally uses a thread pool for multi-day processing\n",
        "\n",
        "### DRY_RUN example\n",
        "\n",
        "Set:\n",
        "- `DRY_RUN=True`\n",
        "- `DRY_RUN_SHOW_SCHEMA=True`\n",
        "\n",
        "Then the notebook will:\n",
        "- print columns and dtypes\n",
        "- `display(df.head(N))`\n",
        "- return success without writing Delta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datetime import timedelta\n",
        "\n",
        "def daterange(start_date: datetime, end_date: datetime):\n",
        "    for n in range((end_date - start_date).days + 1):\n",
        "        yield start_date + timedelta(days=n)\n",
        "\n",
        "def process_day(data_day: datetime) -> int:\n",
        "    df = generate_silver_df_for_day(data_day)\n",
        "    if df.empty:\n",
        "        logging.info(\"[SKIP] %s -> no rows\", data_day.strftime(\"%Y-%m-%d\"))\n",
        "        return 0\n",
        "\n",
        "    if DRY_RUN:\n",
        "        logging.info(\"[DRY_RUN] %s -> %s rows (no write)\", data_day.strftime(\"%Y-%m-%d\"), len(df))\n",
        "        if DRY_RUN_SHOW_SCHEMA:\n",
        "            print(\"Columns:\", list(df.columns))\n",
        "            print(\"Dtypes:\\n\", df.dtypes)\n",
        "        try:\n",
        "            display(df.head(DRY_RUN_SHOW_ROWS))\n",
        "        except Exception:\n",
        "            print(df.head(DRY_RUN_SHOW_ROWS).to_string(index=False))\n",
        "        return len(df)\n",
        "\n",
        "    out_path = silver_table_path(input_stage, TARGET_CLASS, PROJECT_GROUP, PROJECT_NAME)\n",
        "    partition_cols = SILVER_PARTITION_COLS if WRITE_SILVER_PARTITIONED else None\n",
        "\n",
        "    save_df_to_delta(df, out_path, partition_columns=partition_cols, mode=WRITE_MODE)\n",
        "    logging.info(\"[WRITE] %s rows -> %s (partitioned=%s)\", len(df), out_path, bool(partition_cols))\n",
        "\n",
        "    return len(df)\n",
        "\n",
        "def run_core_logic() -> int:\n",
        "    if RESTATE:\n",
        "        start_dt = datetime.strptime(START_RANGE, \"%Y-%m-%d\")\n",
        "        end_dt   = datetime.strptime(END_RANGE, \"%Y-%m-%d\")\n",
        "        days = list(daterange(start_dt, end_dt))\n",
        "    else:\n",
        "        days = [DATA_DAY]\n",
        "\n",
        "    if PARALLEL_DAYS and len(days) > 1 and MAX_WORKERS > 1:\n",
        "        total = 0\n",
        "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "            futures = {ex.submit(process_day, d): d for d in days}\n",
        "            for fut in as_completed(futures):\n",
        "                total += fut.result()\n",
        "        return total\n",
        "\n",
        "    return sum(process_day(d) for d in days)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Finalize and exit\n",
        "\n",
        "This cell returns a JSON payload that pipeline orchestrators can parse.\n",
        "\n",
        "### Success payload\n",
        "```json\n",
        "{\n",
        "  \"status\": \"SUCCEEDED\",\n",
        "  \"projectId\": \"...\",\n",
        "  \"runIntervalUtc\": \"2026-01-03 07:00:00\",\n",
        "  \"rowsProcessed\": 123,\n",
        "  \"errorMessage\": \"\"\n",
        "}\n",
        "```\n",
        "\n",
        "### Failure payload\n",
        "```json\n",
        "{\n",
        "  \"status\": \"FAILED\",\n",
        "  \"projectId\": \"...\",\n",
        "  \"runIntervalUtc\": \"2026-01-03 07:00:00\",\n",
        "  \"rowsProcessed\": 0,\n",
        "  \"errorMessage\": \"Schema mismatch ...\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    processed_rows = run_core_logic()\n",
        "    result = {\n",
        "        \"status\": \"SUCCEEDED\",\n",
        "        \"projectId\": PROJECT_ID,\n",
        "        \"runIntervalUtc\": run_interval_utc,\n",
        "        \"rowsProcessed\": processed_rows,\n",
        "        \"errorMessage\": \"\",\n",
        "    }\n",
        "except Exception as exc:\n",
        "    result = {\n",
        "        \"status\": \"FAILED\",\n",
        "        \"projectId\": PROJECT_ID,\n",
        "        \"runIntervalUtc\": run_interval_utc,\n",
        "        \"rowsProcessed\": 0,\n",
        "        \"errorMessage\": str(exc),\n",
        "    }\n",
        "\n",
        "notebookutils.notebook.exit(json.dumps(result))\n"
      ]
    }
  ]
}
